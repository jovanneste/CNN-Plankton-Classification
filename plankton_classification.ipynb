{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jovanneste/planktonClassification/blob/main/plankton_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWg-9bBqh_Yd"
      },
      "source": [
        "# 2460800V - Deep learning coursework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGBnpu-qrrBQ"
      },
      "source": [
        "# 1. Dataset and problem \n",
        "\n",
        "---\n",
        "\n",
        "> We import all required libraries and our dataset.\n",
        "\n",
        "> At this stage, I manaully look though some of the data to look for class inbalances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjoAk_vxrtG5",
        "outputId": "69802722-f435-46cc-a277-aca1a825c3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-17 12:32:50--  https://www.dropbox.com/s/v2udcnt98miwwrq/plankton.pt?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/v2udcnt98miwwrq/plankton.pt [following]\n",
            "--2023-03-17 12:32:51--  https://www.dropbox.com/s/dl/v2udcnt98miwwrq/plankton.pt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com/cd/0/get/B4Y1yetMEv8PRULJIcZuI2wKuNCmg9wVqFi_f__NHREtCtysQpkNoyWLGDbCwa9ky9ApeDx3NVeiQ0ggBHvFR7yvKKKG6A23TMChi3XHKVuZ061XleP_fJ3VZ7xIz5NIdNa_A0IF8ZY4NkUzn0KFc_ocZtw8GFI4RpyTvHZanQ7qbw/file?dl=1# [following]\n",
            "--2023-03-17 12:32:52--  https://ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com/cd/0/get/B4Y1yetMEv8PRULJIcZuI2wKuNCmg9wVqFi_f__NHREtCtysQpkNoyWLGDbCwa9ky9ApeDx3NVeiQ0ggBHvFR7yvKKKG6A23TMChi3XHKVuZ061XleP_fJ3VZ7xIz5NIdNa_A0IF8ZY4NkUzn0KFc_ocZtw8GFI4RpyTvHZanQ7qbw/file?dl=1\n",
            "Resolving ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com (ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com (ucbbe5dc0a0e5c2876231d8774f7.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 194047719 (185M) [application/binary]\n",
            "Saving to: ‘plankton.pt’\n",
            "\n",
            "plankton.pt         100%[===================>] 185.06M  11.8MB/s    in 14s     \n",
            "\n",
            "2023-03-17 12:33:06 (13.5 MB/s) - ‘plankton.pt’ saved [194047719/194047719]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import random \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import collections\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "from torch.utils.data import WeightedRandomSampler, TensorDataset\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "!wget --no-check-certificate \"https://www.dropbox.com/s/v2udcnt98miwwrq/plankton.pt?dl=1\" -O plankton.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flag = False"
      ],
      "metadata": {
        "id": "DMLXXbWOWp4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5CnfzRJr1tx"
      },
      "source": [
        "# 2. Load data \n",
        "\n",
        "---\n",
        "\n",
        "1. Data augmentation\n",
        "\n",
        "> As our dataset is quite small, I augement some percentage of our data at random. We use transforms to accomplish this, specifically horizontal and vertical flips, rotations, colour transforms and crops. Initailly, I augment 50% of the data.\n",
        "\n",
        "\n",
        "2. Data splitting \n",
        "\n",
        "> We split our augmented data using a 70/10/20 split for training, validation and testing data respectively. We will use both a validation set and cross validation for parameter tuning.\n",
        "\n",
        "> getSplit function \n",
        "\n",
        "> > We create a helper function to see the distribution of the data per set. This will also allow us to find which classes are worst represented. As a sanity check we make sure all three of our classes have rougly the same distibutions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkHnM0QCr2Bh"
      },
      "outputs": [],
      "source": [
        "class DataLoader():\n",
        "  def __init__(self,data):\n",
        "    self.data = data\n",
        "    self.images = data['images']\n",
        "    self.labels = data['labels']\n",
        "    self.shape = self.images.shape\n",
        "\n",
        "  def transform(self, img):\n",
        "    hue = random.uniform(0,0.5)\n",
        "    contrast = random.uniform(0.1,0.8)\n",
        "    degrees = random.randint(5,50)\n",
        "    possible_transforms = [transforms.RandomHorizontalFlip(p=1), transforms.RandomVerticalFlip(p=1), transforms.ColorJitter(brightness=(0.5,0.9), \n",
        "                              contrast=contrast, saturation=0, hue=hue), transforms.RandomRotation(degrees=degrees), transforms.RandomCrop(100, padding=10)]\n",
        "\n",
        "    ts = random.sample(possible_transforms, random.randint(1,len(possible_transforms)))\n",
        "\n",
        "    ts.insert(0, transforms.ToPILImage())\n",
        "    ts.append(transforms.PILToTensor())\n",
        "    ts = transforms.Compose(ts)\n",
        "   \n",
        "    return ts(img)\n",
        "\n",
        "  def augment(self):\n",
        "    for i in range(0, len(self.labels)):\n",
        "      if random.uniform(0,1)<0.8: \n",
        "        transformed_img = torch.reshape(self.transform(self.images[i]), [1,3,100,100])\n",
        "        self.images = torch.cat((self.images, transformed_img))\n",
        "        ls = [int(l) for l in self.labels]\n",
        "        ls.append(self.labels[i])\n",
        "        self.labels = torch.Tensor(ls)\n",
        "    \n",
        "\n",
        "  def split(self):\n",
        "    data_length = len(self.labels)\n",
        "    data_indices = [i for i in range(data_length)]\n",
        "\n",
        "    train_length_ids = random.sample(data_indices, int(data_length*0.0))\n",
        "    [data_indices.remove(x) for x in train_length_ids]\n",
        "\n",
        "    val_length_ids = random.sample(data_indices, int(data_length*0.1))\n",
        "    [data_indices.remove(x) for x in val_length_ids]\n",
        "\n",
        "    TrainData = [list(map(self.images.__getitem__, train_length_ids)), list(map(self.labels.__getitem__, train_length_ids))]\n",
        "    ValData = [list(map(self.images.__getitem__, val_length_ids)), list(map(self.labels.__getitem__, val_length_ids))]\n",
        "    TestData = [list(map(self.images.__getitem__, data_indices)), list(map(self.labels.__getitem__, data_indices))]\n",
        "  \n",
        "    print(\"Length of training split: \", len(TrainData[0]))\n",
        "    print(\"Length of testing split: \", len(TestData[0]))\n",
        "    print(\"Length of validation split: \", len(ValData[0]))\n",
        "\n",
        "    return TrainData, TestData, ValData\n",
        "                    \n",
        "\n",
        "def getSplit(data):\n",
        "  _, labels = data\n",
        "  j=0\n",
        "  uniques, counts = np.unique(labels, return_counts=True)\n",
        "  for i in list(zip(counts * 100 / len(labels))):\n",
        "    print(\"Class \" + str(j) + \": \" + str(round(float(i[0]),2)) + \"%\")\n",
        "    j+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv-aNJx9sU6b",
        "outputId": "153fbca0-7f74-422f-d83c-c475b8425fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training split:  0\n",
            "Length of testing split:  1456\n",
            "Length of validation split:  161\n",
            "Class 0: 7.9%\n",
            "Class 1: 3.64%\n",
            "Class 2: 15.73%\n",
            "Class 3: 6.8%\n",
            "Class 4: 4.26%\n",
            "Class 5: 4.12%\n",
            "Class 6: 5.43%\n",
            "Class 7: 14.35%\n",
            "Class 8: 14.63%\n",
            "Class 9: 5.08%\n",
            "Class 10: 9.89%\n",
            "Class 11: 8.17%\n"
          ]
        }
      ],
      "source": [
        "planktondata = torch.load('plankton.pt')\n",
        "Data = DataLoader(planktondata)\n",
        "if flag:\n",
        "  Data.augment()\n",
        "TrainData, TestData, ValData = Data.split()\n",
        "getSplit(TestData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y55l3oJOsWvc"
      },
      "source": [
        "# 3. Network\n",
        "\n",
        "---\n",
        "\n",
        "### Net1 \n",
        "\n",
        "> Initially I used the architecture shown as Net1. This simple network has 2 convolutional layers and only one pooling layer. \n",
        "\n",
        "\n",
        "### Net2\n",
        "\n",
        "> I improved Net1 and created Net2; a deeper network with batch normalisation and dropout. Using regularisation should help stop the model overfitting.\n",
        "\n",
        "### Loss function \n",
        "\n",
        "> I decide to use Huber loss as opposed to MSE loss as it is less sensitive to outliers and unbalanced data, while still being differentiable and smooth.\n",
        "\n",
        "### Optimiser \n",
        "\n",
        "> I decide to use Adam as mu optimiser as it tends to converge well and is robust to noisy or skewed data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiGgn4lWsYHS"
      },
      "outputs": [],
      "source": [
        "class Net1(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net1, self).__init__()\n",
        "      self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "      self.pool = nn.MaxPool2d(2, 2)\n",
        "      self.fc1 = nn.Linear(64 * 25 * 25, 128)\n",
        "      self.fc2 = nn.Linear(128, 12)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "      x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "      x = x.view(-1, 64 * 25 * 25)\n",
        "      x = nn.functional.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128*12*12, 512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        \n",
        "        self.fc2 = nn.Linear(512, 12)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.maxpool3(x)\n",
        "        \n",
        "        x = x.view(-1, 128*12*12)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "        \n",
        "model = Net2()\n",
        "loss_fn = nn.HuberLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeThO9NvsrEZ"
      },
      "source": [
        "# 3. Weighted dataloader \n",
        "\n",
        "---\n",
        "\n",
        "> Since our data is very unbalanced, I create weighted dataloader which assigns a weight to each sample in our dataset based on its class frequency. The weight assigned to each sample is inversely proportional to its frequency, such that rare classes have higher weights and are therefore sampled more frequently during training. This ensures our CNN sees a balanced representation of all classes during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1Y15RDMsr2M"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "if flag:\n",
        "\n",
        "  def create_weighted_dataloader(data, batch_size):\n",
        "      images = torch.stack(data[0])\n",
        "      labels = torch.tensor(data[1])\n",
        "\n",
        "      class_count = torch.bincount(labels.long())\n",
        "      class_weights = 1.0 / class_count.float()\n",
        "\n",
        "      weights = [class_weights[int(label)] for label in labels]\n",
        "      weights = torch.tensor(weights)\n",
        "      sampler = WeightedRandomSampler(weights, len(weights))\n",
        "\n",
        "      dataset = TensorDataset(images, labels)\n",
        "      dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "      return dataloader\n",
        "\n",
        "\n",
        "  weighted_loader = create_weighted_dataloader(TrainData, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qfG89LvszX4"
      },
      "source": [
        "# 4. Model training and testing functions\n",
        "\n",
        "---\n",
        "\n",
        "> To train our CNN, we create two training functions; one for our general data and one specifically for our weighted data loader.\n",
        "\n",
        "> To test our CNN, we create a function to predict the labels of all the images in our test set, we return all the predictions made alongside the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3h29dpbs0c8"
      },
      "outputs": [],
      "source": [
        "epoch_print_gap = 10\n",
        "\n",
        "# helper functions \n",
        "def labelToTensor(l):\n",
        "  label = [0 if i!=l else 1 for i in range(12)]\n",
        "  return torch.FloatTensor(label)\n",
        "\n",
        "def getImgandLabel(data, batch_size):\n",
        "  data_indices = [i for i in range(len(data[0]))]\n",
        "  ids = random.sample(data_indices, batch_size)\n",
        "  imgs, labels = [data[0][i] for i in ids], [data[1][i] for i in ids]\n",
        "  return imgs, labels\n",
        "\n",
        "\n",
        "def train_model(data, n_epochs, batch_size, verbose=True):\n",
        "    losses = []\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        imgs, labels = getImgandLabel(data, batch_size)\n",
        "        for i in range(len(labels)):\n",
        "\n",
        "            img = torch.reshape(imgs[i], [1,3,100,100])\n",
        "            label = labelToTensor(int(labels[i]))\n",
        "            output = model(img)\n",
        "            loss = loss_fn(output, label)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            loss_train += loss.item()\n",
        "            \n",
        "        if (epoch == 1 and verbose) or (epoch % epoch_print_gap == 0 and verbose):\n",
        "            print('Epoch {}, Training loss {}'.format(epoch, float(loss_train)))\n",
        "            p, a, accuracy = test_model(TestData)\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def train_weighted_model(n_epochs):\n",
        "  losses = []\n",
        "  for epoch in range(n_epochs):\n",
        "    for batch in weighted_loader:\n",
        "        imgs, labels = batch\n",
        "        loss_train = 0.0\n",
        "        for i in range(len(labels)):\n",
        "          img = torch.reshape(imgs[i], [1,3,100,100])\n",
        "          label = labelToTensor(int(labels[i]))\n",
        "          output = model(img)\n",
        "          loss = loss_fn(output, label)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "              \n",
        "          loss_train += loss.item()\n",
        "\n",
        "\n",
        "    if (epoch == 1) or (epoch % epoch_print_gap == 0):\n",
        "      print('Epoch {}, Training loss {}'.format(epoch, float(loss_train)))\n",
        "      p, a, accuracy = test_model(TestData)\n",
        "      losses.append(loss_train)\n",
        "\n",
        "  return losses\n",
        "\n",
        "\n",
        "def test_model(loader):\n",
        "  predicted = []\n",
        "  actual = []\n",
        "  test_loss, correct = 0,0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(loader[0])):\n",
        "      data, target = loader[0][i], labelToTensor(int(loader[1][i]))\n",
        "\n",
        "      img = torch.reshape(data, [1,3,100,100])\n",
        "      label = int(np.argmax(target))\n",
        "\n",
        "      output = model(img)\n",
        "\n",
        "      test_loss += loss_fn(output, target).item()  \n",
        "      pred = int(output.argmax())\n",
        "      predicted.append(pred)\n",
        "      actual.append(label)\n",
        "      \n",
        "      if pred==label:\n",
        "        correct+=1\n",
        "\n",
        "  accuracy = round((correct*100)/len(loader[0]),2)\n",
        "\n",
        "  print(\"Accuracy: \" + str(accuracy)+\"%\")\n",
        "\n",
        "  return predicted, actual, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNn74wcHtu6v"
      },
      "source": [
        "# 4. Optimisation\n",
        "\n",
        "---\n",
        "\n",
        "### Use of validation set \n",
        "\n",
        "> I make use of the validation set to tune the batch number. We find that using a batch size of 32 gives the best results. \n",
        "\n",
        "### Cross validation on training set\n",
        "\n",
        "> I make use of k-fold validation to tune our learning rate. Initally I used a grid search for a wider search however, this took too long to converge. Instead, we use each fold to test a value and find a learning rate of 0.001 gives the best results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSlFlLhquJHL"
      },
      "outputs": [],
      "source": [
        "if flag:\n",
        "  def tuneModel(n_epochs):\n",
        "    for batch in [8, 16, 32, 64, 128]:\n",
        "      model = Net2()\n",
        "      loss_fn = nn.MSELoss()\n",
        "      optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "      losses = train_model(ValData, n_epochs, batch, verbose=False)\n",
        "      predicted, actual, accuracy = test_model(ValData)\n",
        "      print(\"Batch size: \", batch)\n",
        "\n",
        "\n",
        "  tuneModel(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poU6AySfuNuo"
      },
      "outputs": [],
      "source": [
        "# cross validation on training set \n",
        "if flag:\n",
        "  from sklearn.model_selection import KFold\n",
        "\n",
        "  n_folds = 4\n",
        "\n",
        "  def get_k_folds(k):\n",
        "    images = TrainData[0]\n",
        "    labels = TrainData[1]\n",
        "\n",
        "    # shuffle \n",
        "    data = list(zip(images, labels))\n",
        "    random.shuffle(data)\n",
        "    images, labels = zip(*data)\n",
        "\n",
        "    kf = KFold(n_splits=k)\n",
        "    kf.get_n_splits(images)\n",
        "\n",
        "    image_folds_train, labels_folds_train = [], []\n",
        "    image_folds_test, labels_folds_test = [], []\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(images)):\n",
        "      image_folds_train.append(list(map(images.__getitem__, train_index)))\n",
        "      image_folds_test.append(list(map(images.__getitem__, test_index)))\n",
        "      labels_folds_train.append(list(map(labels.__getitem__, train_index)))\n",
        "      labels_folds_test.append(list(map(labels.__getitem__, test_index)))\n",
        "\n",
        "    return image_folds_train, labels_folds_train, image_folds_test, labels_folds_test\n",
        "\n",
        "\n",
        "  image_folds_train, labels_folds_train, image_folds_test, labels_folds_test = get_k_folds(n_folds)\n",
        "\n",
        "  assert len(image_folds_train)==n_folds, \"Wrong number of folds created\"\n",
        "\n",
        "\n",
        "\n",
        "  learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "  for i in range(n_folds):\n",
        "    print(\"Fold \", i)\n",
        "    model = Net2()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rates[i])\n",
        "    print(\"Training\")\n",
        "    losses = train_model([image_folds_train[i], labels_folds_train[i]], 20, 32, verbose=False)\n",
        "\n",
        "    print(\"Testing\")\n",
        "    predicted, actual, accuracy = test_model([image_folds_test[i], labels_folds_test[i]])\n",
        "    print(\"With learning rate=\", learning_rates[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVtmfNZznC5"
      },
      "source": [
        "# 5. Evaluation\n",
        "\n",
        "---\n",
        "\n",
        "### Model results\n",
        "\n",
        "> My first model, Net1, using Adam and Huber loss with the above parameters gave around 40% accuracy on the test data after 50 epochs. Our improved model, Net2, with identical parameters has\n",
        "\n",
        "### Failure cases\n",
        "\n",
        "> I looked at some failure cases and found some images in the dataset were entirely black. I plot a confusion matrix to show  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFIJAjTVzum9"
      },
      "outputs": [],
      "source": [
        "if flag:  \n",
        "  model = Net2()\n",
        "  print(model)\n",
        "  loss_fn = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "  losses = train_weighted_model(50)\n",
        "  # save model after training to github\n",
        "  from google.colab import files\n",
        "  torch.save(model.state_dict(), 'model.pth')\n",
        "  files.download('model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained model from github\n",
        "!git clone https://github.com/jovanneste/deeplearning.git\n",
        "!cd deeplearning/model/\n",
        "!ls\n",
        "checkpoint = torch.load('model.pth', map_location=torch.device('cpu'))\n",
        "print(checkpoint.keys())\n",
        "# model = Net2()\n",
        "# model.load_state_dict(checkpoint)\n",
        "# model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "CP-oZjwUfJdG",
        "outputId": "bda62d96-24d5-4ea1-ba13-c99fe2b167fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'deeplearning' already exists and is not an empty directory.\n",
            "deeplearning  modelweigths.pt  planktonClassification  plankton.pt  sample_data\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-17d994b0eb63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd deeplearning/model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# model = Net2()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEFt6VXXzfuQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "6df3aca1-ed28-4ca6-a12f-a7e14fb380b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-16f33c5227ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcm_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcm_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_plot/confusion_matrix.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, include_values, cmap, xticks_rotation, values_format, ax, colorbar, im_kw, text_kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolorbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         ax.set(\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{cls.__qualname__}.set\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;31m# Artist._update_set_signature_and_docstring() at the end of the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;31m# module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mlack\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprenormalization\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmaintain\u001b[0m \u001b[0mbackcompatibility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \"\"\"\n\u001b[0;32m-> 1223\u001b[0;31m         return self._update_props(\n\u001b[0m\u001b[1;32m   1224\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{cls.__name__}.set() got an unexpected keyword argument \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \"{prop_name!r}\")\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1197\u001b[0m                         raise AttributeError(\n\u001b[1;32m   1198\u001b[0m                             errfmt.format(cls=type(self), prop_name=k))\n\u001b[0;32m-> 1199\u001b[0;31m                     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpchanged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 f\"for the old name will be dropped %(removal)s.\")\n\u001b[1;32m    296\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m# wrapper() must keep the same documented signature as func(): if we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_ticklabels\u001b[0;34m(self, labels, minor, fontdict, **kwargs)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;31m# remove all tick labels, so only error for > 0 labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                     \u001b[0;34m\"The number of FixedLocator locations\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                     \u001b[0;34mf\" ({len(locator.locs)}), usually from a call to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of FixedLocator locations (12), usually from a call to set_ticks, does not match the number of labels (2)."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O59Hjr4aiOBG"
      },
      "source": [
        "# 5. Discussion \n",
        "\n",
        "---\n",
        "\n",
        "### Limitations \n",
        "\n",
        "> blah balh \n",
        "\n",
        "### Future work\n",
        "\n",
        "Some ideas for future work to improve the \n",
        "\n",
        "> Using transfer learning - for example CLIP or ResNet, to improve feature extraction.\n",
        "\n",
        "> Improving model architecture - if I had more time I would have gone back to the start and designed an even deeper model which would have hopefuly learnt better.\n",
        "\n",
        "> Explainability - using methods like LIME or SHAP to get an insight into the model's predictions to show the model is learning features as we would expect.\n",
        "\n",
        "### Self-attention \n",
        "\n",
        "> I tried to improve the Net2 architecture by adding a self-attention aspect. Self-attention will selectively weigh the importance of different feature maps and should improve performance. After some research, I found nn.MultiheadAttention is commonly used to extend CNNs however, my implementation struggled to learn anything useful so I decided not to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3MZ5Hq5iULk"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SelfAttentionCNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=8)\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=128, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=12)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        x = x.permute(2, 3, 0, 1) \n",
        "        # self-attention\n",
        "        x, _ = self.attention(x, x, x) \n",
        "        x = x.permute(2, 3, 0, 1)\n",
        "        \n",
        "        x = F.avg_pool2d(x, kernel_size=x.size()[2:])\n",
        "        \n",
        "        x = x.view(-1, 128)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMuHaqT06axiXOzRzyvRWx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}